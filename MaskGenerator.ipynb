{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MaskGenerator.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/agarwalanant/FyndImageSegmentation/blob/master/MaskGenerator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzAL0DCZqUzz",
        "colab_type": "text"
      },
      "source": [
        "Using Keras for rapid prototyping, if time permits fimal model will be implemented in pytorch or TF 2.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTf44-4GNkJ_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "940ba9a3-3901-40ff-857a-5cf59811ccd9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLPCrDBUv7Eo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "a6dc63d9-85cc-4985-c282-aaf92f39ea03"
      },
      "source": [
        "import keras\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import gzip\n",
        "%matplotlib inline\n",
        "from keras.models import Model\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.layers import Input,Dense,Flatten,Dropout,merge,Reshape,Conv2D,MaxPooling2D,UpSampling2D,Conv2DTranspose\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.models import Model,Sequential\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.optimizers import Adadelta, RMSprop,SGD,Adam\n",
        "from keras import regularizers\n",
        "from keras import backend as K\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, TensorBoard, ModelCheckpoint\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_m44zIihph21",
        "colab_type": "text"
      },
      "source": [
        "**AutoEncoder and Decoder for gerating masks**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAIwgpYvOJFg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip\n",
        "!unzip PennFudanPed.zip -d /content/drive/My\\ \\Drive/imagesegmentdata/Penn_Fudan"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26mfRAwtzb1k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://alphamatting.com/datasets/zip/input_training_lowres.zip\n",
        "!wget http://alphamatting.com/datasets/zip/trimap_training_lowres.zip\n",
        "!wget http://alphamatting.com/datasets/zip/gt_training_lowres.zip    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLS28hU84B-d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip input_training_lowres.zip -d /content/drive/My\\ \\Drive/imagesegmentdata/alphamatting/input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QV4nWTZ24Y8Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !unzip trimap_training_lowres.zip -d /content/drive/My\\ \\Drive/imagesegmentdata/alphamatting\n",
        "!unzip gt_training_lowres.zip  -d /content/drive/My\\ \\Drive/imagesegmentdata/alphamatting/gt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5-WFVg_lxJr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "d6f57626-044f-4f20-d79d-247cca9f1b09"
      },
      "source": [
        "#model parameters\n",
        "batch_size = 1\n",
        "epochs = 200\n",
        "inChannel = 3\n",
        "iml, imh = 500, 500\n",
        "input_img = Input(shape = (iml, imh, inChannel))\n",
        "num_classes = 1\n",
        "batch = 1"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0824 09:40:31.796419 140624720328576 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0824 09:40:31.844732 140624720328576 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbzM_m26mNSH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data Generators\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RXWJEwjxMeE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "24341769-a5dd-4a0e-a301-cdc03a5b1d12"
      },
      "source": [
        "train_generator=train_datagen.flow_from_directory('drive/My Drive/imagesegmentdata/alphamatting/input',\n",
        "                                                 target_size=(iml,imh),\n",
        "                                                 color_mode='rgb',\n",
        "                                                 batch_size=batch,\n",
        "                                                 class_mode='input',\n",
        "                                                 shuffle=True,)\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1. /255)\n",
        "val_generator=train_datagen.flow_from_directory('drive/My Drive/imagesegmentdata/alphamatting/input',\n",
        "                                                 target_size=(iml,imh),\n",
        "                                                 color_mode='rgb',\n",
        "                                                 batch_size=batch,\n",
        "                                                 class_mode='input',\n",
        "                                                 shuffle=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 27 images belonging to 1 classes.\n",
            "Found 27 images belonging to 1 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ferxd3XHqiDa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encoder(input_img):\n",
        "    #encoder\n",
        "\n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img) #28 x 28 x 32\n",
        "    conv1 = BatchNormalization()(conv1)\n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
        "    conv1 = BatchNormalization()(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1) #14 x 14 x 32\n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1) #14 x 14 x 64\n",
        "    conv2 = BatchNormalization()(conv2)\n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
        "    conv2 = BatchNormalization()(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2) #7 x 7 x 64\n",
        "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2) #7 x 7 x 128 (small and thick)\n",
        "    conv3 = BatchNormalization()(conv3)\n",
        "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
        "    conv3 = BatchNormalization()(conv3)\n",
        "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv3) #7 x 7 x 256 (small and thick)\n",
        "    conv4 = BatchNormalization()(conv4)\n",
        "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n",
        "    conv4 = BatchNormalization()(conv4)\n",
        "    return conv4\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rtr-9fnxqkWa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decoder(conv4):    \n",
        "    #decoder\n",
        "    conv5 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv4) #7 x 7 x 128\n",
        "    conv5 = BatchNormalization()(conv5)\n",
        "    conv5 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv5)\n",
        "    conv5 = BatchNormalization()(conv5)\n",
        "    conv6 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv5) #7 x 7 x 64\n",
        "    conv6 = BatchNormalization()(conv6)\n",
        "    conv6 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv6)\n",
        "    conv6 = BatchNormalization()(conv6)\n",
        "    up1 = UpSampling2D((2,2))(conv6) #14 x 14 x 64\n",
        "    conv7 = Conv2D(32, (3, 3), activation='relu', padding='same')(up1) # 14 x 14 x 32\n",
        "    conv7 = BatchNormalization()(conv7)\n",
        "    conv7 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv7)\n",
        "    conv7 = BatchNormalization()(conv7)\n",
        "    up2 = UpSampling2D((2,2))(conv7) # 28 x 28 x 32\n",
        "    decoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(up2) # 28 x 28 x 1\n",
        "    return decoded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNuJKx3hqm1q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        },
        "outputId": "c38ffc17-0d25-4c97-9870-a8e5aa9a7966"
      },
      "source": [
        "autoencoder = Model(input_img, decoder(encoder(input_img)))\n",
        "autoencoder.compile(loss='mean_squared_error', optimizer = RMSprop())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0824 09:40:37.885100 140624720328576 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0824 09:40:37.934525 140624720328576 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0824 09:40:37.935595 140624720328576 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0824 09:40:38.241067 140624720328576 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "W0824 09:40:38.416432 140624720328576 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0824 09:40:39.312196 140624720328576 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
            "\n",
            "W0824 09:40:39.504890 140624720328576 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ndn-y4PrKYI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9fa36bcf-aa8c-4fbc-a9fe-684a9a358e94"
      },
      "source": [
        "autoencoder.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 500, 500, 3)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 500, 500, 32)      896       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 500, 500, 32)      128       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 500, 500, 32)      9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 500, 500, 32)      128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 250, 250, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 250, 250, 64)      18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 250, 250, 64)      256       \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 250, 250, 64)      36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 250, 250, 64)      256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 125, 125, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 125, 125, 128)     73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 125, 125, 128)     512       \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 125, 125, 128)     147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 125, 125, 128)     512       \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 125, 125, 256)     295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 125, 125, 256)     1024      \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 125, 125, 256)     590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 125, 125, 256)     1024      \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 125, 125, 128)     295040    \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 125, 125, 128)     512       \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 125, 125, 128)     147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 125, 125, 128)     512       \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 125, 125, 64)      73792     \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 125, 125, 64)      256       \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 125, 125, 64)      36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 125, 125, 64)      256       \n",
            "_________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2 (None, 250, 250, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 250, 250, 32)      18464     \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 250, 250, 32)      128       \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 250, 250, 32)      9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 250, 250, 32)      128       \n",
            "_________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2 (None, 500, 500, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 500, 500, 3)       867       \n",
            "=================================================================\n",
            "Total params: 1,759,811\n",
            "Trainable params: 1,756,995\n",
            "Non-trainable params: 2,816\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHsK67pgrNVL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=25)\n",
        "early_stopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=25, mode='auto')\n",
        "checkpointer = ModelCheckpoint('/content/drive/My Drive/imagesegmentdata/alphamatting/model.hdf5', mode='min', monitor='val_loss', verbose=1, save_best_only=True)\n",
        "\n",
        "\n",
        "batch_size = batch\n",
        "learning_rate = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhZ3GDHDrTl1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4262ddcb-eda6-496d-a62a-e3014bf1a12a"
      },
      "source": [
        "# checkpoint = ModelCheckpoint('gdrive/My Drive/101/{epoch:02d}-{val_acc:.2f}.hdf5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "# callbacks_list = [checkpoint]\n",
        "step_size_train=train_generator.n//train_generator.batch_size\n",
        "step_size_val=val_generator.n//val_generator.batch_size\n",
        "model_history = []\n",
        "model_history.append(autoencoder.fit_generator(generator=train_generator,\n",
        "                    validation_data=val_generator,\n",
        "                   steps_per_epoch=step_size_train,\n",
        "                                               \n",
        "                    validation_steps = step_size_val,\n",
        "                                               \n",
        "                 epochs=epochs,shuffle = True,\n",
        "          callbacks=[lr_reducer, checkpointer, early_stopper],initial_epoch =0))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "27/27 [==============================] - 189s 7s/step - loss: 0.0229 - val_loss: 0.1678\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.16777, saving model to /content/drive/My Drive/imagesegmentdata/alphamatting/model.hdf5\n",
            "Epoch 2/200\n",
            "27/27 [==============================] - 188s 7s/step - loss: 0.0224 - val_loss: 0.1769\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.16777\n",
            "Epoch 3/200\n",
            "27/27 [==============================] - 200s 7s/step - loss: 0.0219 - val_loss: 0.1190\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.16777 to 0.11905, saving model to /content/drive/My Drive/imagesegmentdata/alphamatting/model.hdf5\n",
            "Epoch 4/200\n",
            "27/27 [==============================] - 202s 7s/step - loss: 0.0211 - val_loss: 0.0927\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.11905 to 0.09267, saving model to /content/drive/My Drive/imagesegmentdata/alphamatting/model.hdf5\n",
            "Epoch 5/200\n",
            "27/27 [==============================] - 189s 7s/step - loss: 0.0212 - val_loss: 0.0551\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.09267 to 0.05514, saving model to /content/drive/My Drive/imagesegmentdata/alphamatting/model.hdf5\n",
            "Epoch 6/200\n",
            "27/27 [==============================] - 211s 8s/step - loss: 0.0181 - val_loss: 0.0628\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.05514\n",
            "Epoch 7/200\n",
            "27/27 [==============================] - 208s 8s/step - loss: 0.0186 - val_loss: 0.0960\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.05514\n",
            "Epoch 8/200\n",
            "27/27 [==============================] - 194s 7s/step - loss: 0.0195 - val_loss: 0.0473\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.05514 to 0.04727, saving model to /content/drive/My Drive/imagesegmentdata/alphamatting/model.hdf5\n",
            "Epoch 9/200\n",
            "27/27 [==============================] - 191s 7s/step - loss: 0.0183 - val_loss: 0.0490\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.04727\n",
            "Epoch 10/200\n",
            "27/27 [==============================] - 191s 7s/step - loss: 0.0174 - val_loss: 0.0445\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.04727 to 0.04452, saving model to /content/drive/My Drive/imagesegmentdata/alphamatting/model.hdf5\n",
            "Epoch 11/200\n",
            "27/27 [==============================] - 188s 7s/step - loss: 0.0184 - val_loss: 0.0448\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.04452\n",
            "Epoch 12/200\n",
            "27/27 [==============================] - 192s 7s/step - loss: 0.0183 - val_loss: 0.0456\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.04452\n",
            "Epoch 13/200\n",
            "27/27 [==============================] - 195s 7s/step - loss: 0.0182 - val_loss: 0.0449\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.04452\n",
            "Epoch 14/200\n",
            "27/27 [==============================] - 196s 7s/step - loss: 0.0135 - val_loss: 0.0541\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.04452\n",
            "Epoch 15/200\n",
            "27/27 [==============================] - 213s 8s/step - loss: 0.0170 - val_loss: 0.0530\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.04452\n",
            "Epoch 16/200\n",
            "27/27 [==============================] - 214s 8s/step - loss: 0.0181 - val_loss: 0.0287\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.04452 to 0.02873, saving model to /content/drive/My Drive/imagesegmentdata/alphamatting/model.hdf5\n",
            "Epoch 17/200\n",
            "27/27 [==============================] - 211s 8s/step - loss: 0.0167 - val_loss: 0.0203\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.02873 to 0.02033, saving model to /content/drive/My Drive/imagesegmentdata/alphamatting/model.hdf5\n",
            "Epoch 18/200\n",
            "27/27 [==============================] - 211s 8s/step - loss: 0.0179 - val_loss: 0.0346\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.02033\n",
            "Epoch 19/200\n",
            "27/27 [==============================] - 210s 8s/step - loss: 0.0144 - val_loss: 0.0157\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.02033 to 0.01571, saving model to /content/drive/My Drive/imagesegmentdata/alphamatting/model.hdf5\n",
            "Epoch 20/200\n",
            "27/27 [==============================] - 213s 8s/step - loss: 0.0179 - val_loss: 0.0267\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.01571\n",
            "Epoch 21/200\n",
            "27/27 [==============================] - 212s 8s/step - loss: 0.0133 - val_loss: 0.0398\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.01571\n",
            "Epoch 22/200\n",
            "27/27 [==============================] - 213s 8s/step - loss: 0.0171 - val_loss: 0.0171\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.01571\n",
            "Epoch 23/200\n",
            "27/27 [==============================] - 224s 8s/step - loss: 0.0181 - val_loss: 0.0134\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.01571 to 0.01343, saving model to /content/drive/My Drive/imagesegmentdata/alphamatting/model.hdf5\n",
            "Epoch 24/200\n",
            "27/27 [==============================] - 228s 8s/step - loss: 0.0158 - val_loss: 0.0138\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.01343\n",
            "Epoch 25/200\n",
            "27/27 [==============================] - 228s 8s/step - loss: 0.0165 - val_loss: 0.0329\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.01343\n",
            "Epoch 26/200\n",
            "27/27 [==============================] - 228s 8s/step - loss: 0.0150 - val_loss: 0.0212\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.01343\n",
            "Epoch 27/200\n",
            "27/27 [==============================] - 228s 8s/step - loss: 0.0153 - val_loss: 0.0185\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.01343\n",
            "Epoch 28/200\n",
            "27/27 [==============================] - 228s 8s/step - loss: 0.0164 - val_loss: 0.0340\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.01343\n",
            "Epoch 29/200\n",
            "27/27 [==============================] - 228s 8s/step - loss: 0.0163 - val_loss: 0.0191\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.01343\n",
            "Epoch 30/200\n",
            "27/27 [==============================] - 229s 8s/step - loss: 0.0146 - val_loss: 0.0161\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.01343\n",
            "Epoch 31/200\n",
            "27/27 [==============================] - 228s 8s/step - loss: 0.0137 - val_loss: 0.0095\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.01343 to 0.00952, saving model to /content/drive/My Drive/imagesegmentdata/alphamatting/model.hdf5\n",
            "Epoch 32/200\n",
            "27/27 [==============================] - 227s 8s/step - loss: 0.0140 - val_loss: 0.0125\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.00952\n",
            "Epoch 33/200\n",
            "27/27 [==============================] - 228s 8s/step - loss: 0.0150 - val_loss: 0.0102\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.00952\n",
            "Epoch 34/200\n",
            "27/27 [==============================] - 228s 8s/step - loss: 0.0139 - val_loss: 0.0167\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.00952\n",
            "Epoch 35/200\n",
            "27/27 [==============================] - 228s 8s/step - loss: 0.0127 - val_loss: 0.0090\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.00952 to 0.00900, saving model to /content/drive/My Drive/imagesegmentdata/alphamatting/model.hdf5\n",
            "Epoch 36/200\n",
            "27/27 [==============================] - 228s 8s/step - loss: 0.0154 - val_loss: 0.0160\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.00900\n",
            "Epoch 37/200\n",
            "27/27 [==============================] - 228s 8s/step - loss: 0.0149 - val_loss: 0.0124\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.00900\n",
            "Epoch 38/200\n",
            "27/27 [==============================] - 228s 8s/step - loss: 0.0144 - val_loss: 0.0107\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.00900\n",
            "Epoch 39/200\n",
            "27/27 [==============================] - 228s 8s/step - loss: 0.0138 - val_loss: 0.0115\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.00900\n",
            "Epoch 40/200\n",
            "27/27 [==============================] - 228s 8s/step - loss: 0.0141 - val_loss: 0.0131\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.00900\n",
            "Epoch 41/200\n",
            "27/27 [==============================] - 228s 8s/step - loss: 0.0127 - val_loss: 0.0106\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.00900\n",
            "Epoch 42/200\n",
            "27/27 [==============================] - 228s 8s/step - loss: 0.0120 - val_loss: 0.0088\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.00900 to 0.00884, saving model to /content/drive/My Drive/imagesegmentdata/alphamatting/model.hdf5\n",
            "Epoch 43/200\n",
            "27/27 [==============================] - 228s 8s/step - loss: 0.0143 - val_loss: 0.0137\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.00884\n",
            "Epoch 44/200\n",
            "27/27 [==============================] - 228s 8s/step - loss: 0.0119 - val_loss: 0.0096\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.00884\n",
            "Epoch 45/200\n",
            "27/27 [==============================] - 228s 8s/step - loss: 0.0137 - val_loss: 0.0086\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.00884 to 0.00863, saving model to /content/drive/My Drive/imagesegmentdata/alphamatting/model.hdf5\n",
            "Epoch 46/200\n",
            "27/27 [==============================] - 228s 8s/step - loss: 0.0136 - val_loss: 0.0070\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.00863 to 0.00699, saving model to /content/drive/My Drive/imagesegmentdata/alphamatting/model.hdf5\n",
            "Epoch 47/200\n",
            "27/27 [==============================] - 228s 8s/step - loss: 0.0122 - val_loss: 0.0079\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.00699\n",
            "Epoch 48/200\n",
            "27/27 [==============================] - 228s 8s/step - loss: 0.0137 - val_loss: 0.0086\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.00699\n",
            "Epoch 49/200\n",
            "27/27 [==============================] - 228s 8s/step - loss: 0.0134 - val_loss: 0.0088\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.00699\n",
            "Epoch 50/200\n",
            "27/27 [==============================] - 228s 8s/step - loss: 0.0139 - val_loss: 0.0093\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.00699\n",
            "Epoch 51/200\n",
            "27/27 [==============================] - 228s 8s/step - loss: 0.0116 - val_loss: 0.0146\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.00699\n",
            "Epoch 52/200\n",
            "13/27 [=============>................] - ETA: 1:32 - loss: 0.0119"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZlG4JhFyTe_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "img = cv2.imread('/content/drive/My Drive/imagesegmentdata/alphamatting/input/input/GT01.png')\n",
        "img = cv2.resize(img,(500,500))\n",
        "img = np.reshape(img,[1,500,500,3])\n",
        "ans = autoencoder.predict(img)\n",
        "ans = ans.reshape(500,500,3)\n",
        "cv2.imwrite('/content/drive/My Drive/imagesegmentdata/alphamatting/t.png',ans)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}