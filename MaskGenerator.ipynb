{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MaskGenerator.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/agarwalanant/FyndImageSegmentation/blob/master/MaskGenerator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzAL0DCZqUzz",
        "colab_type": "text"
      },
      "source": [
        "Using Keras for rapid prototyping, if time permits fimal model will be implemented in pytorch or TF 2.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTf44-4GNkJ_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "37d2b814-3f6b-4402-c590-e1d9e5663efa"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLPCrDBUv7Eo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "0ccae6d2-61b6-47b1-c325-092d3d0197f6"
      },
      "source": [
        "import keras\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import gzip\n",
        "%matplotlib inline\n",
        "from keras.models import Model\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.layers import Input,Dense,Flatten,Dropout,merge,Reshape,Conv2D,MaxPooling2D,UpSampling2D,Conv2DTranspose\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.models import Model,Sequential\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.optimizers import Adadelta, RMSprop,SGD,Adam\n",
        "from keras import regularizers\n",
        "from keras import backend as K\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, TensorBoard, ModelCheckpoint\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_m44zIihph21",
        "colab_type": "text"
      },
      "source": [
        "**AutoEncoder and Decoder for gerating masks**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAIwgpYvOJFg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip\n",
        "!unzip PennFudanPed.zip -d /content/drive/My\\ \\Drive/imagesegmentdata/Penn_Fudan"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26mfRAwtzb1k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://alphamatting.com/datasets/zip/input_training_lowres.zip\n",
        "!wget http://alphamatting.com/datasets/zip/trimap_training_lowres.zip\n",
        "!wget http://alphamatting.com/datasets/zip/gt_training_lowres.zip    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLS28hU84B-d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip input_training_lowres.zip -d /content/drive/My\\ \\Drive/imagesegmentdata/alphamatting/input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QV4nWTZ24Y8Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !unzip trimap_training_lowres.zip -d /content/drive/My\\ \\Drive/imagesegmentdata/alphamatting\n",
        "!unzip gt_training_lowres.zip  -d /content/drive/My\\ \\Drive/imagesegmentdata/alphamatting/gt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5-WFVg_lxJr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model parameters\n",
        "batch_size = 1\n",
        "epochs = 2\n",
        "inChannel = 3\n",
        "iml, imh = 500, 500\n",
        "input_img = Input(shape = (iml, imh, inChannel))\n",
        "num_classes = 1\n",
        "batch = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbzM_m26mNSH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data Generators\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RXWJEwjxMeE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "6cc4dfe3-078f-49de-8949-e89735402b6b"
      },
      "source": [
        "train_generator=train_datagen.flow_from_directory('drive/My Drive/imagesegmentdata/alphamatting/input',\n",
        "                                                 target_size=(iml,imh),\n",
        "                                                 color_mode='rgb',\n",
        "                                                 batch_size=batch,\n",
        "                                                 class_mode='input',\n",
        "                                                 shuffle=True,)\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1. /255)\n",
        "val_generator=train_datagen.flow_from_directory('drive/My Drive/imagesegmentdata/alphamatting/input',\n",
        "                                                 target_size=(iml,imh),\n",
        "                                                 color_mode='rgb',\n",
        "                                                 batch_size=batch,\n",
        "                                                 class_mode='input',\n",
        "                                                 shuffle=True)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 27 images belonging to 1 classes.\n",
            "Found 27 images belonging to 1 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ferxd3XHqiDa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encoder(input_img):\n",
        "    #encoder\n",
        "\n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img) #28 x 28 x 32\n",
        "    conv1 = BatchNormalization()(conv1)\n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
        "    conv1 = BatchNormalization()(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1) #14 x 14 x 32\n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1) #14 x 14 x 64\n",
        "    conv2 = BatchNormalization()(conv2)\n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
        "    conv2 = BatchNormalization()(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2) #7 x 7 x 64\n",
        "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2) #7 x 7 x 128 (small and thick)\n",
        "    conv3 = BatchNormalization()(conv3)\n",
        "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
        "    conv3 = BatchNormalization()(conv3)\n",
        "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv3) #7 x 7 x 256 (small and thick)\n",
        "    conv4 = BatchNormalization()(conv4)\n",
        "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n",
        "    conv4 = BatchNormalization()(conv4)\n",
        "    return conv4\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rtr-9fnxqkWa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decoder(conv4):    \n",
        "    #decoder\n",
        "    conv5 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv4) #7 x 7 x 128\n",
        "    conv5 = BatchNormalization()(conv5)\n",
        "    conv5 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv5)\n",
        "    conv5 = BatchNormalization()(conv5)\n",
        "    conv6 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv5) #7 x 7 x 64\n",
        "    conv6 = BatchNormalization()(conv6)\n",
        "    conv6 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv6)\n",
        "    conv6 = BatchNormalization()(conv6)\n",
        "    up1 = UpSampling2D((2,2))(conv6) #14 x 14 x 64\n",
        "    conv7 = Conv2D(32, (3, 3), activation='relu', padding='same')(up1) # 14 x 14 x 32\n",
        "    conv7 = BatchNormalization()(conv7)\n",
        "    conv7 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv7)\n",
        "    conv7 = BatchNormalization()(conv7)\n",
        "    up2 = UpSampling2D((2,2))(conv7) # 28 x 28 x 32\n",
        "    decoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(up2) # 28 x 28 x 1\n",
        "    return decoded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNuJKx3hqm1q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "autoencoder = Model(input_img, decoder(encoder(input_img)))\n",
        "autoencoder.compile(loss='mean_squared_error', optimizer = RMSprop())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ndn-y4PrKYI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "489623cd-ab0a-4861-b350-a60b727d02cc"
      },
      "source": [
        "autoencoder.summary()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         (None, 500, 500, 3)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_46 (Conv2D)           (None, 500, 500, 32)      896       \n",
            "_________________________________________________________________\n",
            "batch_normalization_43 (Batc (None, 500, 500, 32)      128       \n",
            "_________________________________________________________________\n",
            "conv2d_47 (Conv2D)           (None, 500, 500, 32)      9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_44 (Batc (None, 500, 500, 32)      128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 250, 250, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_48 (Conv2D)           (None, 250, 250, 64)      18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_45 (Batc (None, 250, 250, 64)      256       \n",
            "_________________________________________________________________\n",
            "conv2d_49 (Conv2D)           (None, 250, 250, 64)      36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_46 (Batc (None, 250, 250, 64)      256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 125, 125, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_50 (Conv2D)           (None, 125, 125, 128)     73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_47 (Batc (None, 125, 125, 128)     512       \n",
            "_________________________________________________________________\n",
            "conv2d_51 (Conv2D)           (None, 125, 125, 128)     147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_48 (Batc (None, 125, 125, 128)     512       \n",
            "_________________________________________________________________\n",
            "conv2d_52 (Conv2D)           (None, 125, 125, 256)     295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_49 (Batc (None, 125, 125, 256)     1024      \n",
            "_________________________________________________________________\n",
            "conv2d_53 (Conv2D)           (None, 125, 125, 256)     590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_50 (Batc (None, 125, 125, 256)     1024      \n",
            "_________________________________________________________________\n",
            "conv2d_54 (Conv2D)           (None, 125, 125, 128)     295040    \n",
            "_________________________________________________________________\n",
            "batch_normalization_51 (Batc (None, 125, 125, 128)     512       \n",
            "_________________________________________________________________\n",
            "conv2d_55 (Conv2D)           (None, 125, 125, 128)     147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_52 (Batc (None, 125, 125, 128)     512       \n",
            "_________________________________________________________________\n",
            "conv2d_56 (Conv2D)           (None, 125, 125, 64)      73792     \n",
            "_________________________________________________________________\n",
            "batch_normalization_53 (Batc (None, 125, 125, 64)      256       \n",
            "_________________________________________________________________\n",
            "conv2d_57 (Conv2D)           (None, 125, 125, 64)      36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_54 (Batc (None, 125, 125, 64)      256       \n",
            "_________________________________________________________________\n",
            "up_sampling2d_7 (UpSampling2 (None, 250, 250, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_58 (Conv2D)           (None, 250, 250, 32)      18464     \n",
            "_________________________________________________________________\n",
            "batch_normalization_55 (Batc (None, 250, 250, 32)      128       \n",
            "_________________________________________________________________\n",
            "conv2d_59 (Conv2D)           (None, 250, 250, 32)      9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_56 (Batc (None, 250, 250, 32)      128       \n",
            "_________________________________________________________________\n",
            "up_sampling2d_8 (UpSampling2 (None, 500, 500, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_60 (Conv2D)           (None, 500, 500, 3)       867       \n",
            "=================================================================\n",
            "Total params: 1,759,811\n",
            "Trainable params: 1,756,995\n",
            "Non-trainable params: 2,816\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHsK67pgrNVL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=8)\n",
        "early_stopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=8, mode='auto')\n",
        "checkpointer = ModelCheckpoint('/content/drive/My Drive/imagesegmentdata/alphamatting/model.hdf5', mode='min', monitor='val_loss', verbose=1, save_best_only=True)\n",
        "\n",
        "\n",
        "batch_size = batch\n",
        "learning_rate = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhZ3GDHDrTl1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "f9db4896-007a-479f-d161-d38ac52b1724"
      },
      "source": [
        "# checkpoint = ModelCheckpoint('gdrive/My Drive/101/{epoch:02d}-{val_acc:.2f}.hdf5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "# callbacks_list = [checkpoint]\n",
        "step_size_train=train_generator.n//train_generator.batch_size\n",
        "step_size_val=val_generator.n//val_generator.batch_size\n",
        "model_history = []\n",
        "model_history.append(autoencoder.fit_generator(generator=train_generator,\n",
        "                    validation_data=val_generator,\n",
        "                   steps_per_epoch=step_size_train,\n",
        "                                               \n",
        "                    validation_steps = step_size_val,\n",
        "                                               \n",
        "                 epochs=epochs,shuffle = True,\n",
        "          callbacks=[lr_reducer, checkpointer, early_stopper],initial_epoch =0))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "27/27 [==============================] - 233s 9s/step - loss: 0.0630 - val_loss: 0.0738\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.07376, saving model to /content/drive/My Drive/imagesegmentdata/alphamatting/model.hdf5\n",
            "Epoch 2/2\n",
            "27/27 [==============================] - 227s 8s/step - loss: 0.0378 - val_loss: 0.3652\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.07376\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZlG4JhFyTe_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "img = cv2.imread('/content/drive/My Drive/imagesegmentdata/alphamatting/input/input/GT01.png')\n",
        "img = cv2.resize(img,(500,500))\n",
        "img = np.reshape(img,[1,500,500,3])\n",
        "ans = autoencoder.predict(img)\n",
        "ans = ans.reshape(500,500,3)\n",
        "cv2.imwrite('/content/drive/My Drive/imagesegmentdata/alphamatting/t.png',ans)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}